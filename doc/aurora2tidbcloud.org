#+OPTIONS: \n:t
* Background
  In this post, I will explain the demo to migrate aws aurora database to [[https://tidbcloud.com/][TiDB Cloud]] with [[https://github.com/aws/aws-sdk-go-v2][AWS SDK]] and TiDB Cloud OpenAPI with minimum manual work. It contains resource generation, full data migration and incremental replication.
* Architecture
    [[./png/aurora2tidbcloud/Aurora2TiDBCloud.01.png]]
    
  + AWS aurora cluster setup with golang SDK
    According to provided config file, it generates the Aurora Cluster using [[https://github.com/aws/aws-sdk-go-v2][golang sdk]].
  + EC2 node generation for workstation and DM cluster
    Generate EC2 nodes for workstation and DM clusters. The workstation is used to run tiup to deploy DM cluster, data comparison, table creation and data generation. The DM cluster is used for data replication from aws aurora to TiDB Cloud.
  + TiDB Cloud setup
    The TiDB Cloud API is used to generate TiDB Cloud cluster. Please find the [[https://docs.pingcap.com/tidbcloud/api-overview][OpenAPI]] interface and [[https://github.com/luyomo/tidbcloud-sdk-go-v1][tidb cloud golang sdk]] for your reference.
  + Setup Private Link between TiDB Cloud and workstation/aurora (Interactive operation)
    Need interactive operation to setup privatelink between workstation/DM cluster and TiDB Cloud since it has not been provided by TiDB Cloud. 
  + Test table creation and test data generation
    Create one table and insert few rows for test. 
  + Take binlog position from AWS aurora
    Take the binlog position before taking snapshot, with which the DM replication task will be created. 
  + AWS aurora snapshot taking
    Take the aurora snapshot to extract data to S3. 
  + Export snapshot to S3 parauet data
    Export the snapshot to S3 parquet which is supported to be imported by [[https://docs.pingcap.com/tidbcloud/import-parquet-files][TiDB Cloud]].
  + Data import to TiDB Cloud
    TiDB Cloud supports the data import API which make the data integration easier. 
  + DM setup for replication
    So far DM on TiDB Cloud API has not been supported. To make the demo simpler, deploy the DM cluster on AWS premise. 
  + Data comparison between TiDB Cloud and aurora
    The [[https://docs.pingcap.com/tidb/dev/sync-diff-inspector-overview][sync-diff-inspector]] is used to compare between Aurora and TiDB Cloud to make sure that the data has been migrated successfully.


  [[./png/aurora2tidbcloud/Aurora2TiDBCloud.02.png]]

* Demo execution
  #+BEGIN_SRC
pi@local$ more /tmp/aurora2tidbcloud.yaml
 workstation:
   cidr: 172.82.0.0/16
   instance_type: c5.2xlarge
   keyname: key-name                              # public key name
   keyfile: /home/pi/.ssh/private-key-name        # private key name
   username: admin
   imageid: ami-07d02ee1eeb0c996c
   volumeSize: 100
   #shared: false
 aurora:
   cidr: 172.84.0.0/16
   instance_type: db.r5.large
   db_parameter_family_group: aurora-mysql5.7
   engine: aurora-mysql
   engine_version: 5.7.mysql_aurora.2.11.2
   db_username: admin
   db_password: 1234Abcd
   public_accessible_flag: false
   s3backup_folder: s3://jay-data/aurora-export/   # s3 directory for data export
 aws_topo_configs:
   general:
     # debian os
     imageid: ami-07d02ee1eeb0c996c                # Default image id for EC2
     keyname: jay-us-east-01                       # Public key to access the EC2 instance
     keyfile: /home/pi/.ssh/jay-us-east-01.pem     # Private key ti access the EC2 instance
     cidr: 172.83.0.0/16                           # The cidr for the VPC
     instance_type: m5.2xlarge                     # Default instance type
     tidb_version: v6.5.2                          # TiDB version
     excluded_az:                                  # The AZ to be excluded for the subnets
       - us-east-1e
     network_type: nat
   dm-master:
     instance_type: t2.small                       # Instance type for dm master
     count: 1                                      # Number of dm master node to be deployed
   dm-worker:
     instance_type: t2.small                       # Instance type for dm worker
     count: 1                                      # Number of dm worker node to be deployed
 tidb_cloud:
   tidbcloud_project_id: 1111113089206752222       # The project id in the tidb cloud in which tidb cluster is to be created.
   cloud_provider: AWS
   cluster_type: DEDICATED
   region: us-east-1
   components:
     tidb:
       node_quantity: 1
       node_size: 2C8G
     tikv:
       node_quantity: 3
       node_size: 2C8G
       storage_size_gib: 200
   ip_access_list:
     cidr: 0.0.0.0/0
     description: Data migration from aurora to TiDB Test
   port: 4000
   user: root
   password: 1234Abcd
   databases:
     - test01
     - test02
pi@local$./bin/aws aurora2tidbcloud deploy aurora2tidbcloud /tmp/aurora2tidbcloud.yaml
Parallel Main step ... Echo: Create TransitGateway ... ...
... ...
  #+END_SRC
* Manual setup
** binlog is enabled
   Make sure that the binlog is enabled in the aurora for the incremental replication.
 [[./png/aurora2tidbcloud/01.aurora.01.png]]
   Memo the binlog position from which DM starts the replication.
 [[./png/aurora2tidbcloud/01.aurora.02.png]]
** DB Size
 [[./png/aurora2tidbcloud/01.aurora.03.png]]
 [[./png/aurora2tidbcloud/01.aurora.04.png]]
* Take aurora snapshot and export to S3
** IAM preparation for data export
   Please refer to the [[https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-export-snapshot.html][official document]] for IAM preparation which is required by data export to S3.
*** policy
    #+BEGIN_SRC sh
      OhMyTiUP$aws iam create-policy  --policy-name Aurora2TiDBCloud-policy --policy-document '{
          "Version": "2012-10-17",                                                                                                                         
          "Statement": [
              {
                  "Sid": "ExportPolicy",
                  "Effect": "Allow",
                  "Action": [
                      "s3:PutObject*",
                      "s3:ListBucket",
                      "s3:GetObject*",
                      "s3:DeleteObject*",
                      "s3:GetBucketLocation"
                  ],
                  "Resource": [
                      "arn:aws:s3:::ossinsight-data",
                      "arn:aws:s3:::ossinsight-data/migration2tidbcloud/*"
                  ]
              }
          ]
      }'
      
    #+END_SRC
*** Role
   #+BEGIN_SRC sh
OhMyTiUP$aws iam create-role  --role-name Aurora2TiDBCloud-role --assume-role-policy-document '{            
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
            "Service": "export.rds.amazonaws.com"
          },
         "Action": "sts:AssumeRole"
       }
     ]
   }'

   #+END_SRC
*** Policy attachment
   #+BEGIN_SRC sh
OhMyTiUP$aws iam attach-role-policy  --policy-arn arn:aws:iam::123456789098:policy/Aurora2TiDBCloud-policy  --role-name Aurora2TiDBCloud-role
   #+END_SRC
** Data export to S3
 [[./png/aurora2tidbcloud/02.aurora.snapshot.01.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.02.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.03.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.04.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.05.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.06.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.07.png]]
** Enable server-side encryption
   TiDB need the encryption for data import
 [[./png/aurora2tidbcloud/02.aurora.snapshot.08.png]]
** TiDB setup
   Here the process how to setup the TiDB is skiped. 
 [[./png/aurora2tidbcloud/03.tidbcloud.01.png]]
** Setup VPC Peering between TiDB Cloud and workstation/DM Cluster/Aurora
   + workstation is used to general operation like schema copy/data comparision and DM setup etc. Workstation has to access TiDB Cloud, Aurora and DM Clusters
   + DM cluster is used to sync the data from Aurora to TiDB Cloud. So the DM has to access both TiDB Cloud and Aurora.
 [[./png/aurora2tidbcloud/03.tidbcloud.02.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.03.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.04.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.05.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.06.png]]
* Schema copy using mysqldump
 [[./png/aurora2tidbcloud/04.tidbcloud.schema.01.png]]
 [[./png/aurora2tidbcloud/04.tidbcloud.schema.02.png]]
* Data import to TiDB Cloud from S3
** Get the account id and external id for IAM setting. Please refer to the [[https://docs.pingcap.com/tidbcloud/config-s3-and-gcs-access][official document]] for IAM setup
 [[./png/aurora2tidbcloud/05.tidbcloud.import.01.png]]
   #+BEGIN_SRC
OhMyTiUP$aws iam create-policy  --policy-name Aurora2TiDBCloud-policy --policy-document '{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ExportPolicy",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject*",
                "s3:ListBucket",
                "s3:GetObject*",
                "s3:DeleteObject*",
                "s3:GetBucketLocation"
            ],
            "Resource": [
                "arn:aws:s3:::ossinsight-data",
                "arn:aws:s3:::ossinsight-data/migration2tidbcloud/*"
            ]
        }
    ]
}'
    #+END_SRC
    
    #+BEGIN_SRC


aws iam create-role  --role-name Aurora2TiDBCloud-role  --assume-role-policy-document '{
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
            "Service": "export.rds.amazonaws.com"
          },
         "Action": "sts:AssumeRole"
       }
     ] 
   }'

    #+END_SRC

    #+BEGIN_SRC
aws iam attach-role-policy  --policy-arn arn:aws:iam::729581434105:policy/Aurora2TiDBCloud-policy  --role-name Aurora2TiDBCloud-role
   #+END_SRC
** Data import from GUI.
   It takes about 44 minutes to complete 250GB data.
 [[./png/aurora2tidbcloud/05.tidbcloud.import.02.png]]
 [[./png/aurora2tidbcloud/05.tidbcloud.import.03.png]]
 [[./png/aurora2tidbcloud/05.tidbcloud.import.04.png]]
** Data comparison between TiDB Cloud and Aurora snapshot
   In this example, the data comparison is done between Aurora and TiDB Cloud directly. In the prod migration phase, not to impact the application, recommend to restore the snapshot to new instance for data comparison.
 [[./png/aurora2tidbcloud/05.tidbcloud.import.05.png]]
 [[./png/aurora2tidbcloud/05.tidbcloud.import.06.png]]
* Start DM SYNC process
** DM Cluster config
 [[./png/aurora2tidbcloud/06.dm.setup.01.png]]
** DM Source config file
 [[./png/aurora2tidbcloud/06.dm.setup.02.png]]
** DM task config file
   The binlog-name and binlog-pos is taken before aurora snapshot for data consistency.
 [[./png/aurora2tidbcloud/06.dm.setup.03.png]]
** Check the DM status
 [[./png/aurora2tidbcloud/06.dm.setup.04.png]]
 [[./png/aurora2tidbcloud/06.dm.setup.05.png]]
* Run the sysbench against Aurora
 [[./png/aurora2tidbcloud/07.dm.sync.01.png]]
** Compare the data between Aurora and TiDB Cloud after DM sync
   This step is optional for prod migration. If the app needs to switch to new DB ASAP, this step can be taken only for testing. Recommend users to figure out some business query to verify the data bewteen TiDB Cloud and aurora.
 [[./png/aurora2tidbcloud/07.dm.sync.02.png]]
* Stop the DM sync task from Aurora to TiDB Cloud
 [[./png/aurora2tidbcloud/07.dm.sync.03.png]]
 [[./png/aurora2tidbcloud/07.dm.sync.04.png]]
* Start the TiCDC sync from TiDB Cloud to Aurora
 [[./png/aurora2tidbcloud/08.ticdc.sync.01.png]]
 [[./png/aurora2tidbcloud/08.ticdc.sync.02.png]]
** Run sysbench against TiDB Cloud to simulate the APP switch
 [[./png/aurora2tidbcloud/08.ticdc.sync.03.png]]
** After 1 hour, compare the data again
   This process is only used on the test environment. In the prod, the final comparison is not required.
 [[./png/aurora2tidbcloud/08.ticdc.sync.04.png]]


* Reference
** Cluster installation
   #+BEGIN_SRC sh
OhMyTiUP$embed/examples/aws/aurora2tidbcloud.yaml
workstation:
  cidr: 172.82.0.0/16
  #instance_type: m5.2xlarge
  instance_type: c5.4xlarge
  keyname: jay-us-east-01
  keyfile: /home/pi/.ssh/jay-us-east-01.pem
  username: admin
  imageid: ami-07d02ee1eeb0c996c
  volumeSize: 100
  #shared: false
aurora:
  cidr: 172.84.0.0/16
  instance_type: db.r5.large
  db_parameter_family_group: aurora-mysql5.7
  engine: aurora-mysql
  engine_version: 5.7.mysql_aurora.2.10.1
  db_username: admin
  db_password: 1234Abcd
  public_accessible_flag: false
aws_topo_configs:
  general:
    # debian os
    imageid: ami-07d02ee1eeb0c996c                # Default image id for EC2
    keyname: jay-us-east-01                       # Public key to access the EC2 instance
    keyfile: /home/pi/.ssh/jay-us-east-01.pem     # Private key ti access the EC2 instance
    cidr: 172.83.0.0/16                           # The cidr for the VPC
    instance_type: m5.2xlarge                     # Default instance type
    tidb_version: v6.1.0                          # TiDB version
    excluded_az:                                  # The AZ to be excluded for the subnets
      - us-east-1e
    enable_nat: true                              # The flag to decide whether the nat is created in the TiDB VPC
  dm-master:
    instance_type: c5.2xlarge                     # Instance type for dm master
    count: 3                                      # Number of dm master node to be deployed
  dm-worker:
    instance_type: c5.2xlarge                     # Instance type for dm worker
    count: 3                                      # Number of dm worker node to be deployed
tidb_cloud:
  host: private-tidb.ixezxfbrz7x.clusters.tidb-cloud.com
  port: 4000
  user: root
  password: 1234Abcd
  databases:
    - test01
    - test02
   #+END_SRC
** Deploy Aurora DB
   #+BEGIN_SRC
./bin/aws aurora deploy aurora2tidbtest embed/examples/aws/aurora.yaml
   #+END_SRC
** Generate test data(Need to check data volume)
   #+BEGIN_SRC
$mysqlslap generate test data
$ SELECT 
     table_schema as `Database`, 
     table_name AS `Table`, 
     round(((data_length + index_length) / 1024 / 1024), 2) `Size in MB` 
FROM information_schema.TABLES 
ORDER BY (data_length + index_length) DESC;
   #+END_SRC
   


   tiup dmctl --master-addr  172.83.1.119:8261  operate-source create /opt/tidb/dm-source.yml

   tiup dmctl --master-addr 172.83.1.119:8261 start-task /opt/tidb/dm-task.yml


   #+BEGIN_SRC
aurora$ mysqldump -h aurora2tidbcloudtest-rdsdbinstance1-uabt2ganvcep.cxmxisy1o2a2.us-east-1.rds.amazonaws.com -u admin -P 3306 -p1234Abcd --no-data test01 > /tmp/schema.ddl
   #+END_SRC


* Purpose
** Verify the data import using API
** Clean the data migration process and document
** Verify the on-duplicate - ignore feature
** Understand the DM feature and make the demo using DM on Cloud
* Topic
** Background
Last week I need to verify on-duplicate=ignore(one config option of DM) of the data replication from aurora to TiDB Cloud. The default replication behavour is as below:
*** Default behavour
     + Aurora Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  1 |     1 |
       |  2 |     2 |
     #+TBLSPAN: A1..B1::C1..D1

     + TiDB Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  2 |   200 |
       |  3 |   300 |
     #+TBLSPAN: A1..B1::C1..D1

     + After full data replication
       #+ATTR_HTML: :border 2 :rules all :frame border
       | Aurora(Source) |       | TiDB Cloud(Target) |       | Comment                     |
       |             PK | Value |                 PK | Value |                             |
       |----------------+-------+--------------------+-------+-----------------------------|
       |              1 |     1 |                  1 |     1 | Copied                      |
       |              2 |     2 |                  2 |     2 | Replace from aurora to TiDb |
       |                |       |                  3 |   300 |                             |
     #+TBLSPAN: A1..B1::C1..D1
*** on-duplicate=ignore
     + Aurora Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  1 |     1 |
       |  2 |     2 |
     #+TBLSPAN: A1..B1::C1..D1

     + TiDB Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  2 |   200 |
       |  3 |   300 |
     #+TBLSPAN: A1..B1::C1..D1

     + After full data replication
       #+ATTR_HTML: :border 2 :rules all :frame border
       | Aurora(Source) |       | TiDB Cloud(Target) |       | Comment                     |
       |             PK | Value |                 PK | Value |                             |
       |----------------+-------+--------------------+-------+-----------------------------|
       |              1 |     1 |                  1 |     1 | Copied                      |
       |              2 |     2 |                  2 |     2 | Replace from aurora to TiDb |
       |                |       |                  3 |   300 |                             |
     #+TBLSPAN: A1..B1::C1..D1
    
  If you have aurora and TiDB Cluster available for this test, please ignore this article. But if you do not have all the resources, please use it to shorted the verification time. What you need to do is to define one yaml file and create all the resources using one command. 
** Who is the reader?
   This topic is used to verifiy the on-duplicate = ignore.
   We have to setup the minimum sdk to control the cost.
   
   + The pre sale to setup the demo. Why do we need it? When we have to setup the environment for DM troubleshooting, we need it for verification without too much time.
     - Setup the TiDB Cloud
     - Setup Aurora Database
     - Setup DM Cluster
   + OpenAPI sdk verification
   
  Setup the replication demo from Aurora to TiDB Cloud within 30 minutes
** Show the sdk of tidb cloud
* Todo
** Clean the aurora setup scripts through the stack info and start the aurora from command
** Include one workstation
** Create one TiDB Cloud for test
   + Create cluster
   + Check Cluster status
   + Data import using API script
   + Check data import status
   + Create endpoint from API
   + Accept private endpoing from workstation
** Setup the DM cluster for replication

   #+BEGIN_SRC
name: "aurora2tidbcloud"
task-mode: full
meta-schema: "dm_meta"

target-database:
  host: "privatelink-tidb.oyhkulpcx68.clusters.tidb-cloud.com"
  port: 4000
  user: "root"
  password: "1234Abcd"
  max-allowed-packet: 67108864

routes:
  route-rule-0:
    schema-pattern: "test01"
    target-schema: "test01"
  route-rule-1:
    schema-pattern: "test02"
    target-schema: "test02"

mysql-instances:
  -
    source-id: "aurora2tidbcloud"
    block-allow-list:  "instance"
    loader-config-name: "global"
    route-rules: [ "route-rule-0", "route-rule-1"]

block-allow-list:
  instance:
    do-dbs: ["test01", "test02"]

loaders:
  global:
    pool-size: 16
    dir: "./dumped_data"
    import-mode: "logical"
    on-duplicate-logical: "ignore"
   #+END_SRC

* TODOList
** Full case
*** Destroy command to include
**** DONE TiDB Cloud
**** DONE S3 backup
**** DONE RDS snapshot
*** Make progress visible
**** TODO Aurora setup
**** TODO TiDB cloud setup
**** TODO Aurora database snapshot taken
**** TODO Data S3 export
*** List all the time taken on each process
*** List all the AWS Resource
    + RDS
    + S3
    + KMS
    + Subnet
    + VPC
*** DONE Prompt improvment
*** DONE Comment removal
*** DONE How to resume vpce if the status is pending
*** DONE extract_oracle_instance_info.go rename
*** DONE Convert S3/bucket as parameter
*** DONE TiDB Version as parameter
*** TiDB components
    + EC2 instance creation
    + TiDB Cloud
    + Aurora database
    + Snapshot taken
    + Export snapshot to S3
    + Import data into TiDB Cloud from S3
    + DM deployment
** Replication without Aurora
** Replication without TiDB Cloud and aurora
* Issues
** READ: packet for query is too large. Try adjusting the 'max_allowed_packet' variable on the server\" , Workaround: Please execute `query-status` to check status."
   Set max_allowed_packet to 1073741824
** READ: Error 1142: SELECT, LOCK TABLES command denied to user 'admin'@'172.83.3.108' for table 'accounts'"
#+BEGIN_SRC
... ...
mysql-instances:
  -
    source-id: "aurora2tidbcloud"
    block-allow-list:  "instance"
    loader-config-name: "global"
    route-rules: [ "route-rule-0", "route-rule-1"]

block-allow-list:
  instance:
    do-dbs: ["test01", "test02"]
... ...
#+END_SRC

** parse mydumper metadata error: open /home/admin/dm/dm-deploy/dm-worker-8262/dumped_data.aurora2tidbcloud/metadata: no such file or directory, metadata: not found
rm /home/admin/dm/dm-deploy/dm-worker-8262/dumped_data.aurora2tidbcloud
/home/admin/.tiup/bin/tiup dmctl --master-addr 172.83.5.22:8261 start-task /tmp/dm-task.yml --remove-meta

* Issue
** Error: operation error EC2: CreateVpcEndpoint, https response error StatusCode: 400, RequestID: 682fa180-fa53-4ad3-a901-2946bec8310c, api error InvalidParameter: The VPC endpoint service com.amazonaws.vpce.us-east-1.vpce-svc-06d591e2487736f5a does not support the availability zone of the subnet: subnet-068a6987f91221918.
Not all the zones are supported for the VPC endpoint service. Please use the below command to determine what zone are supportted. 
#+BEGIN_SRC
{
    "ServiceDetails": [
        { ... ...
            "AvailabilityZones": [
                "us-east-1a",
                "us-east-1b",
                "us-east-1c"
            ], 
    ... ...
}
#+END_SRC

* Get Auroa info
** Select the resion for aurora
** Select aurora database 
** Check DB configuration
   + binlog format
** Setup the vpc peering or transti gateway for connection
