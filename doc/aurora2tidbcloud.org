* Architecture
  + Take Aurora GTID
  + Take aurora database snapshot
  + Convert aurora snapshot to S3(Parquet)
  + Import snapshot data into TiDB Cloud from S3
  + Setup DM to replicate data from Aurora to TiDB Cloud(using GTID)
  + Stop DM replication
  + Wait for replication completeness
  + Start the reverse TiCDC reverse replication from TiDB Cloud to Aurora
  [[./png/aurora2tidbcloud/Aurora2TiDBCloud.01.png]]
  [[./png/aurora2tidbcloud/Aurora2TiDBCloud.02.png]]

* Aurora environment
** binlog is enabled
   Make sure that the binlog is enabled in the aurora for the incremental replication.
 [[./png/aurora2tidbcloud/01.aurora.01.png]]
   Memo the binlog position from which DM starts the replication.
 [[./png/aurora2tidbcloud/01.aurora.02.png]]
** DB Size
 [[./png/aurora2tidbcloud/01.aurora.03.png]]
 [[./png/aurora2tidbcloud/01.aurora.04.png]]
* Take aurora snapshot and export to S3
** IAM preparation for data export
   Please refer to the [[https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-export-snapshot.html][official document]] for IAM preparation which is required by data export to S3.
*** policy
    #+BEGIN_SRC sh
      OhMyTiUP$aws iam create-policy  --policy-name Aurora2TiDBCloud-policy --policy-document '{
          "Version": "2012-10-17",                                                                                                                         
          "Statement": [
              {
                  "Sid": "ExportPolicy",
                  "Effect": "Allow",
                  "Action": [
                      "s3:PutObject*",
                      "s3:ListBucket",
                      "s3:GetObject*",
                      "s3:DeleteObject*",
                      "s3:GetBucketLocation"
                  ],
                  "Resource": [
                      "arn:aws:s3:::ossinsight-data",
                      "arn:aws:s3:::ossinsight-data/migration2tidbcloud/*"
                  ]
              }
          ]
      }'
      
    #+END_SRC
*** Role
   #+BEGIN_SRC sh
OhMyTiUP$aws iam create-role  --role-name Aurora2TiDBCloud-role --assume-role-policy-document '{            
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
            "Service": "export.rds.amazonaws.com"
          },
         "Action": "sts:AssumeRole"
       }
     ]
   }'

   #+END_SRC
*** Policy attachment
   #+BEGIN_SRC sh
OhMyTiUP$aws iam attach-role-policy  --policy-arn arn:aws:iam::123456789098:policy/Aurora2TiDBCloud-policy  --role-name Aurora2TiDBCloud-role
   #+END_SRC
** Data export to S3
 [[./png/aurora2tidbcloud/02.aurora.snapshot.01.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.02.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.03.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.04.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.05.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.06.png]]
 [[./png/aurora2tidbcloud/02.aurora.snapshot.07.png]]
** Enable server-side encryption
   TiDB need the encryption for data import
 [[./png/aurora2tidbcloud/02.aurora.snapshot.08.png]]
** TiDB setup
   Here the process how to setup the TiDB is skiped. 
 [[./png/aurora2tidbcloud/03.tidbcloud.01.png]]
** Setup VPC Peering between TiDB Cloud and workstation/DM Cluster/Aurora
   + workstation is used to general operation like schema copy/data comparision and DM setup etc. Workstation has to access TiDB Cloud, Aurora and DM Clusters
   + DM cluster is used to sync the data from Aurora to TiDB Cloud. So the DM has to access both TiDB Cloud and Aurora.
 [[./png/aurora2tidbcloud/03.tidbcloud.02.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.03.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.04.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.05.png]]
 [[./png/aurora2tidbcloud/03.tidbcloud.06.png]]
* Schema copy using mysqldump
 [[./png/aurora2tidbcloud/04.tidbcloud.schema.01.png]]
 [[./png/aurora2tidbcloud/04.tidbcloud.schema.02.png]]
* Data import to TiDB Cloud from S3
** Get the account id and external id for IAM setting. Please refer to the [[https://docs.pingcap.com/tidbcloud/config-s3-and-gcs-access][official document]] for IAM setup
 [[./png/aurora2tidbcloud/05.tidbcloud.import.01.png]]
   #+BEGIN_SRC
OhMyTiUP$aws iam create-policy  --policy-name Aurora2TiDBCloud-policy --policy-document '{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ExportPolicy",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject*",
                "s3:ListBucket",
                "s3:GetObject*",
                "s3:DeleteObject*",
                "s3:GetBucketLocation"
            ],
            "Resource": [
                "arn:aws:s3:::ossinsight-data",
                "arn:aws:s3:::ossinsight-data/migration2tidbcloud/*"
            ]
        }
    ]
}'
    #+END_SRC
    
    #+BEGIN_SRC


aws iam create-role  --role-name Aurora2TiDBCloud-role  --assume-role-policy-document '{
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
            "Service": "export.rds.amazonaws.com"
          },
         "Action": "sts:AssumeRole"
       }
     ] 
   }'

    #+END_SRC

    #+BEGIN_SRC
aws iam attach-role-policy  --policy-arn arn:aws:iam::729581434105:policy/Aurora2TiDBCloud-policy  --role-name Aurora2TiDBCloud-role
   #+END_SRC
** Data import from GUI.
   It takes about 44 minutes to complete 250GB data.
 [[./png/aurora2tidbcloud/05.tidbcloud.import.02.png]]
 [[./png/aurora2tidbcloud/05.tidbcloud.import.03.png]]
 [[./png/aurora2tidbcloud/05.tidbcloud.import.04.png]]
** Data comparison between TiDB Cloud and Aurora snapshot
   In this example, the data comparison is done between Aurora and TiDB Cloud directly. In the prod migration phase, not to impact the application, recommend to restore the snapshot to new instance for data comparison.
 [[./png/aurora2tidbcloud/05.tidbcloud.import.05.png]]
 [[./png/aurora2tidbcloud/05.tidbcloud.import.06.png]]
* Start DM SYNC process
** DM Cluster config
 [[./png/aurora2tidbcloud/06.dm.setup.01.png]]
** DM Source config file
 [[./png/aurora2tidbcloud/06.dm.setup.02.png]]
** DM task config file
   The binlog-name and binlog-pos is taken before aurora snapshot for data consistency.
 [[./png/aurora2tidbcloud/06.dm.setup.03.png]]
** Check the DM status
 [[./png/aurora2tidbcloud/06.dm.setup.04.png]]
 [[./png/aurora2tidbcloud/06.dm.setup.05.png]]
* Run the sysbench against Aurora
 [[./png/aurora2tidbcloud/07.dm.sync.01.png]]
** Compare the data between Aurora and TiDB Cloud after DM sync
   This step is optional for prod migration. If the app needs to switch to new DB ASAP, this step can be taken only for testing. Recommend users to figure out some business query to verify the data bewteen TiDB Cloud and aurora.
 [[./png/aurora2tidbcloud/07.dm.sync.02.png]]
* Stop the DM sync task from Aurora to TiDB Cloud
 [[./png/aurora2tidbcloud/07.dm.sync.03.png]]
 [[./png/aurora2tidbcloud/07.dm.sync.04.png]]
* Start the TiCDC sync from TiDB Cloud to Aurora
 [[./png/aurora2tidbcloud/08.ticdc.sync.01.png]]
 [[./png/aurora2tidbcloud/08.ticdc.sync.02.png]]
** Run sysbench against TiDB Cloud to simulate the APP switch
 [[./png/aurora2tidbcloud/08.ticdc.sync.03.png]]
** After 1 hour, compare the data again
   This process is only used on the test environment. In the prod, the final comparison is not required.
 [[./png/aurora2tidbcloud/08.ticdc.sync.04.png]]


* Reference
** Cluster installation
   #+BEGIN_SRC sh
OhMyTiUP$embed/examples/aws/aurora2tidbcloud.yaml
workstation:
  cidr: 172.82.0.0/16
  #instance_type: m5.2xlarge
  instance_type: c5.4xlarge
  keyname: jay-us-east-01
  keyfile: /home/pi/.ssh/jay-us-east-01.pem
  username: admin
  imageid: ami-07d02ee1eeb0c996c
  volumeSize: 100
  #shared: false
aurora:
  cidr: 172.84.0.0/16
  instance_type: db.r5.large
  db_parameter_family_group: aurora-mysql5.7
  engine: aurora-mysql
  engine_version: 5.7.mysql_aurora.2.10.1
  db_username: admin
  db_password: 1234Abcd
  public_accessible_flag: false
aws_topo_configs:
  general:
    # debian os
    imageid: ami-07d02ee1eeb0c996c                # Default image id for EC2
    keyname: jay-us-east-01                       # Public key to access the EC2 instance
    keyfile: /home/pi/.ssh/jay-us-east-01.pem     # Private key ti access the EC2 instance
    cidr: 172.83.0.0/16                           # The cidr for the VPC
    instance_type: m5.2xlarge                     # Default instance type
    tidb_version: v6.1.0                          # TiDB version
    excluded_az:                                  # The AZ to be excluded for the subnets
      - us-east-1e
    enable_nat: true                              # The flag to decide whether the nat is created in the TiDB VPC
  dm-master:
    instance_type: c5.2xlarge                     # Instance type for dm master
    count: 3                                      # Number of dm master node to be deployed
  dm-worker:
    instance_type: c5.2xlarge                     # Instance type for dm worker
    count: 3                                      # Number of dm worker node to be deployed
tidb_cloud:
  host: private-tidb.ixezxfbrz7x.clusters.tidb-cloud.com
  port: 4000
  user: root
  password: 1234Abcd
  databases:
    - test01
    - test02
   #+END_SRC
** Deploy Aurora DB
   #+BEGIN_SRC
./bin/aws aurora deploy aurora2tidbtest embed/examples/aws/aurora.yaml
   #+END_SRC
** Generate test data(Need to check data volume)
   #+BEGIN_SRC
$mysqlslap generate test data
$ SELECT 
     table_schema as `Database`, 
     table_name AS `Table`, 
     round(((data_length + index_length) / 1024 / 1024), 2) `Size in MB` 
FROM information_schema.TABLES 
ORDER BY (data_length + index_length) DESC;
   #+END_SRC
   


   tiup dmctl --master-addr  172.83.1.119:8261  operate-source create /opt/tidb/dm-source.yml

   tiup dmctl --master-addr 172.83.1.119:8261 start-task /opt/tidb/dm-task.yml


   #+BEGIN_SRC
aurora$ mysqldump -h aurora2tidbcloudtest-rdsdbinstance1-uabt2ganvcep.cxmxisy1o2a2.us-east-1.rds.amazonaws.com -u admin -P 3306 -p1234Abcd --no-data test01 > /tmp/schema.ddl
   #+END_SRC


* Purpose
** Verify the data import using API
** Clean the data migration process and document
** Verify the on-duplicate - ignore feature
** Understand the DM feature and make the demo using DM on Cloud
* Topic
** Background
Last week I need to verify on-duplicate=ignore(one config option of DM) of the data replication from aurora to TiDB Cloud. The default replication behavour is as below:
*** Default behavour
     + Aurora Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  1 |     1 |
       |  2 |     2 |
     #+TBLSPAN: A1..B1::C1..D1

     + TiDB Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  2 |   200 |
       |  3 |   300 |
     #+TBLSPAN: A1..B1::C1..D1

     + After full data replication
       #+ATTR_HTML: :border 2 :rules all :frame border
       | Aurora(Source) |       | TiDB Cloud(Target) |       | Comment                     |
       |             PK | Value |                 PK | Value |                             |
       |----------------+-------+--------------------+-------+-----------------------------|
       |              1 |     1 |                  1 |     1 | Copied                      |
       |              2 |     2 |                  2 |     2 | Replace from aurora to TiDb |
       |                |       |                  3 |   300 |                             |
     #+TBLSPAN: A1..B1::C1..D1
*** on-duplicate=ignore
     + Aurora Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  1 |     1 |
       |  2 |     2 |
     #+TBLSPAN: A1..B1::C1..D1

     + TiDB Data
       #+ATTR_HTML: :border 2 :rules all :frame border
       | PK | Value |
       |----+-------|
       |  2 |   200 |
       |  3 |   300 |
     #+TBLSPAN: A1..B1::C1..D1

     + After full data replication
       #+ATTR_HTML: :border 2 :rules all :frame border
       | Aurora(Source) |       | TiDB Cloud(Target) |       | Comment                     |
       |             PK | Value |                 PK | Value |                             |
       |----------------+-------+--------------------+-------+-----------------------------|
       |              1 |     1 |                  1 |     1 | Copied                      |
       |              2 |     2 |                  2 |     2 | Replace from aurora to TiDb |
       |                |       |                  3 |   300 |                             |
     #+TBLSPAN: A1..B1::C1..D1
    
  If you have aurora and TiDB Cluster available for this test, please ignore this article. But if you do not have all the resources, please use it to shorted the verification time. What you need to do is to define one yaml file and create all the resources using one command. 
** Who is the reader?
   This topic is used to verifiy the on-duplicate = ignore.
   We have to setup the minimum sdk to control the cost.
   
   + The pre sale to setup the demo. Why do we need it? When we have to setup the environment for DM troubleshooting, we need it for verification without too much time.
     - Setup the TiDB Cloud
     - Setup Aurora Database
     - Setup DM Cluster
   + OpenAPI sdk verification
   
  Setup the replication demo from Aurora to TiDB Cloud within 30 minutes
** Show the sdk of tidb cloud
* Todo
** Clean the aurora setup scripts through the stack info and start the aurora from command
** Include one workstation
** Create one TiDB Cloud for test
   + Create cluster
   + Check Cluster status
   + Data import using API script
   + Check data import status
   + Create endpoint from API
   + Accept private endpoing from workstation
** Setup the DM cluster for replication

   #+BEGIN_SRC
name: "aurora2tidbcloud"
task-mode: full
meta-schema: "dm_meta"

target-database:
  host: "privatelink-tidb.oyhkulpcx68.clusters.tidb-cloud.com"
  port: 4000
  user: "root"
  password: "1234Abcd"
  max-allowed-packet: 67108864

routes:
  route-rule-0:
    schema-pattern: "test01"
    target-schema: "test01"
  route-rule-1:
    schema-pattern: "test02"
    target-schema: "test02"

mysql-instances:
  -
    source-id: "aurora2tidbcloud"
    block-allow-list:  "instance"
    loader-config-name: "global"
    route-rules: [ "route-rule-0", "route-rule-1"]

block-allow-list:
  instance:
    do-dbs: ["test01", "test02"]

loaders:
  global:
    pool-size: 16
    dir: "./dumped_data"
    import-mode: "logical"
    on-duplicate-logical: "ignore"
   #+END_SRC

* TODOList
** Destroy command to include
   + TiDB Cloud
   + S3 backup
   + RDS snapshot
** Make progress visible
   + Aurora setup
   + TiDB cloud setup
   + Aurora database snapshot taken
   + Data S3 export
** List all the time taken on each process
** List all the AWS Resource
   + RDS
   + S3
   + KMS
   + Subnet
   + VPC
** Prompt improvment
* Issues
** READ: packet for query is too large. Try adjusting the 'max_allowed_packet' variable on the server\" , Workaround: Please execute `query-status` to check status."
   Set max_allowed_packet to 1073741824
** READ: Error 1142: SELECT, LOCK TABLES command denied to user 'admin'@'172.83.3.108' for table 'accounts'"
#+BEGIN_SRC
... ...
mysql-instances:
  -
    source-id: "aurora2tidbcloud"
    block-allow-list:  "instance"
    loader-config-name: "global"
    route-rules: [ "route-rule-0", "route-rule-1"]

block-allow-list:
  instance:
    do-dbs: ["test01", "test02"]
... ...
#+END_SRC

** parse mydumper metadata error: open /home/admin/dm/dm-deploy/dm-worker-8262/dumped_data.aurora2tidbcloud/metadata: no such file or directory, metadata: not found
rm /home/admin/dm/dm-deploy/dm-worker-8262/dumped_data.aurora2tidbcloud
/home/admin/.tiup/bin/tiup dmctl --master-addr 172.83.5.22:8261 start-task /tmp/dm-task.yml --remove-meta

* Issue
** Error: operation error EC2: CreateVpcEndpoint, https response error StatusCode: 400, RequestID: 682fa180-fa53-4ad3-a901-2946bec8310c, api error InvalidParameter: The VPC endpoint service com.amazonaws.vpce.us-east-1.vpce-svc-06d591e2487736f5a does not support the availability zone of the subnet: subnet-068a6987f91221918.
Not all the zones are supported for the VPC endpoint service. Please use the below command to determine what zone are supportted. 
#+BEGIN_SRC
{
    "ServiceDetails": [
        { ... ...
            "AvailabilityZones": [
                "us-east-1a",
                "us-east-1b",
                "us-east-1c"
            ], 
    ... ...
}
#+END_SRC
