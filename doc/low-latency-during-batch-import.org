#+OPTIONS: \n:t
#+OPTIONS: ^:nil
#+TITLE: TiKV nodes isolation between batch and online transaction

* OLTP isolation from Batch
  [[./png/placementrule/isolation-batch-oltp.01.png]]
  [[./png/placementrule/isolation-batch-oltp.02.png]]
* TiDB Cluster generation with node labels
** Cluster deployment
  Use the below config to generate TiDB Cluster with TiKV nodes labels. Three TiKV nodes are group together as batch nodes. And the remaining three are grouped as online nodes. 
  #+BEGIN_SRC
OhMyTiUP$ more /tmp/aws-nodes-tidb.yaml
 workstation:
   imageid: ami-07d02ee1eeb0c996c                  # Workstation EC2 instance
   keyname: key name                               # Public key for workstation instance deployment
   keyfile: /home/pi/.ssh/local-pricate-key.pem    # Private key to access the workstation
   volumeSize: 100                                 # disk size in the workstation
   enable_monitoring: enabled                      # enable the moniroting on the workstation
   instance_type: c5.2xlarge
   cidr: 172.81.0.0/16
 aws_topo_configs:
   general:
     # debian os
     imageid: ami-07d02ee1eeb0c996c                # Default image id for EC2
     keyname: jay-us-east-01                       # Public key to access the EC2 instance
     keyfile: /home/pi/.ssh/jay-us-east-01.pem
     cidr: 172.83.0.0/16
     tidb_version: v6.1.0
     excluded_az:                                  # The AZ to be excluded for the subnets
       - us-east-1e
   pd:
     instance_type: c5.2xlarge
     count: 3
   tidb:
     instance_type: c5.2xlarge
     count: 2
   tikv:
     labels:
     - name: db_type
       values:
       - value: online
         machine_type: standard
       - value: batch
         machine_type: standard
     machine_types:
       -
         name: standard
         modal_value:
           instance_type: c5.2xlarge
           count: 3
           volumeSize: 300
           volumeType: gp3
           iops: 3000
OhMyTiUP$ ./bin/aws tidb deploy placementruletest /tmp/aws-nodes-tidb.yaml
  #+END_SRC
  [[./png/placementrule/placementrule.01.png]]
  [[./png/placementrule/placementrule.02.png]]
** List all the resources
   #+BEGIN_SRC
OhMyTiUP$ ./bin/aws tidb list placementruletest
   #+END_SRC
   [[./png/placementrule/placementrule.03.png]]
   [[./png/placementrule/placementrule.04.png]]
** Check the placement rule labels
   [[./png/placementrule/placementrule.05.png]]

* Command description
** measure-latency
   There are three sub command for the latency measurement.
   + prepare - Prepare the test environment including database/table/placement rule generation and data preparation
   + run     - Run the actual test
   + cleanup - Cleanup all the resources (todo)
  [[./png/placementrule/placementrule.06.png]]
** measure-latency prepare
   This command is used for test preparation. Including:
   + batch database creation
   + batch table creation
   + batch table(ontime) sample data download and import to DB
   + Placement rule policy creation(mode: partition)
   + sysbench database creation
   + sysbench table creation
   + sysbench customization module upload

   [[./png/placementrule/placementrule.11.png]]

** measure-latency run
*** Simple TiKV mode
    In the mode, all the TiKV nodes are share by the batch and online application. Sometimes heavy batch might impact the online transaction even though there is no table confilict. Especially the log apply duration is impacted heavily if the batch is heavy.
*** Placment rule policy TiKV mode
    In this mode, the TiKV nodes are grouped to two. Online and batch. The online group nodes are used only by sysbench while the batch nodes are only used by batch. With placement rule, the TiKV resources isolation are achieved.
    
   [[./png/placementrule/placementrule.12.png]]
* Latency impact test
** Scenario: Common TiDB Cluster without resource isolation
*** Preparation
#+BEGIN_SRC
OhMyTiUP$ ./bin/aws tidb perf resource-isolation prepare placementruletest --sysbench-num-tables 10  --tikv-mode simple --ssh-user admin --identity-file /home/pi/.ssh/jay-us-east-01.pem
        
#+END_SRC
[[./png/placementrule/placementrule.07.png]]   
*** Run test
#+BEGIN_SRC

OhMyTiUP$./bin/aws tidb perf resource-isolation run placementruletest --sysbench-num-tables 10 --batch-size x,50000 --repeats 10 --sysbench-plugin-name tidb_oltp_insert_simple --ssh-user admin --identity-file /home/pi/.ssh/jay-us-east-01.pem
#+END_SRC
[[./png/placementrule/placementrule.08.png]]

As the result from the above example, because of the heavy batch impact, the latency and qps are impacted compared to the case where only sysbench is running.
** Scenario: Common TiDB Cluster without resource isolation
*** Preparation
    #+BEGIN_SRC
OhMyTiUP$./bin/aws tidb measure-latency prepare placementruletest --sysbench-execution-time 40 --sysbench-num-tables 10  --tikv-mode partition --ssh-user admin --identity-file /home/pi/.ssh/private-key.pem
    #+END_SRC
[[./png/placementrule/placementrule.09.png]]
** Run test against cluster with batch/online isolated
   
    #+BEGIN_SRC
./bin/aws tidb measure-latency run placementruletest --repeats 2 --trans-interval 200 --batch-size x,50000 --ssh-user admin --identity-file /home/pi/.ssh/private-key.pem
    #+END_SRC
[[./png/placementrule/placementrule.10.png]]
The sysbench is not impacted by the batch too much. In other words, from the result the resources between batch and online application are isolated from each other.
* How to simulate the batch import
Use the below flow to simulate the heavy batch process.
  + Create the ontime and ontime01 table. Please refer to [[https://github.com/ClickHouse/ClickHouse/blob/master/docs/en/getting-started/example-datasets/ontime.md][Clickhouse-sample-data]]
  + Import one Month data into ontime01
  + Insert into ontime select * from ontim01 limit 10000

                        
