#+OPTIONS: ^:nil
#+OPTIONS: \t:t
* Overview
 #+attr_html: :width 1000px
 [[./png/mongo2kafka2TiDB/01.overview.png]]
* Cluster Deployment
  + Mongo
  + kafka
  + TiDB
** Deployment
   The whole process takes about 30 minutes in my raspberry servers for your reference.
  + MongoDB = 3 replication node + 1 config server + 1 mongosh agent
    #+Caption: Mongo DB topo
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.01.mongo.config.png]]
  + Kafka cluster = 1 broker + 1 schema registry + 1 connector + 1 rest service
    #+Caption: Kafka cluster topo
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.02.kafka.config.png]]
  + TiDB Cluster = 1 pd + 1 TiKV + 1 TiDB
    #+Caption: TiDB Cluster topo
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.03.tidb.config.png]]
  + Deployment
    #+Caption: Cluster Deployment
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.04.deployment.png]]
** Confirmation
*** Cluster
    #+Caption: Cluster list
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.05.01.confirm.png]]
    #+Caption: Cluster list
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.05.02.confirm.png]]
*** Mongo
    Check mongo replication cluster access and replica set.
    #+Caption: Mongo DB access confirmation
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.06.01.mongo.png]]
    #+Caption: Mongo DB access confirmation
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.06.02.mongo.png]]
*** kafka
    List all the partition to check kafka's cluster
    #+Caption: kafka topic
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.07.kafka.png]]
*** TiDB
    #+Caption: TiDB access
    #+attr_html: :width 1000px
    [[./png/mongo2kafka2TiDB/02.08.tidb.png]]

* Source Connector Deployment
** Source sink install
   Install mongo source connector through confluent-hub-components which is used to mongo CDC.
   #+BEGIN_SRC
connector$ sudo confluent-hub install debezium/debezium-connector-mongodb:1.9.7
The component can be installed in any of the following Confluent Platform installations: 
  1. / (installed rpm/deb package) 
  2. / (where this tool is installed) 
Choose one of these to continue the installation (1-2):1
Do you want to install this into /usr/share/confluent-hub-components? (yN) y
Component's license: 
Apache 2.0 
https://github.com/debezium/debezium/blob/master/LICENSE.txt 
I agree to the software license agreement (yN) y
You are about to install 'debezium-connector-mongodb' from Debezium Community, as published on Confluent Hub. 
Do you want to continue? (yN) y
Downloading component Debezium MongoDB CDC Connector 1.9.7, provided by Debezium Community from Confluent Hub and installing into /usr/share/confluent-hub-components 
Detected Worker's configs: 
  1. Standard: /etc/kafka/connect-distributed.properties 
  2. Standard: /etc/kafka/connect-standalone.properties 
  3. Standard: /etc/schema-registry/connect-avro-distributed.properties 
  4. Standard: /etc/schema-registry/connect-avro-standalone.properties 
  5. Used by Connect process with PID 20522: /etc/kafka/connect-distributed.properties 
Do you want to update all detected configs? (yN) y
Adding installation directory to plugin path in the following files: 
  /etc/kafka/connect-distributed.properties 
  /etc/kafka/connect-standalone.properties 
  /etc/schema-registry/connect-avro-distributed.properties 
  /etc/schema-registry/connect-avro-standalone.properties 
  /etc/kafka/connect-distributed.properties 
 
Completed
connector$sudo systemctl restart confluent-kafka-connect
   #+END_SRC
** connector creation
   + source connector creation
     #+BEGIN_SRC
workstation$ more /tmp/source.mongo.json
{
  "name": "source.mongo",
  "config": {
    "connector.class": "io.debezium.connector.mongodb.MongoDbConnector", 
    "mongodb.hosts": "rs0/172.84.3.17:27017",
    "mongodb.name": "mongo2tidb"
  }
}
worksation$ curl -H "Content-Type: application/json" http://172.83.1.135:8083/connectors -d @"/tmp/source.mongo.json" | jq
     #+END_SRC
     #+Caption: New topic
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/03.01.source.connect.png]]
   + Source connector status check
     #+BEGIN_SRC
worksation$ curl http://172.83.1.135:8083/connectors/source.mongo/status | jq
     #+END_SRC
     #+Caption: Status check
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/03.02.source.status.png]]
** Data insert into Mongo
   Insert first row into Mongo to check the replication. At the same time, the first kafka topic is generated.
   #+Caption: Mongo DB Data insert
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/03.03.mongo.insert.png]]
** topic confirmation
   Use command to check the generated kafka topic from mongo collection.
   #+Caption: New topic
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/03.04.kafka.topic.png]]
** Data confirmation
   Consume topic from console for contents check. From the output, the json format data is viewed.
   #+BEGIN_SRC
workstation$ kafka-console-consumer --bootstrap-server 172.83.1.232:9092 --topic mongo2tidb.test.test01 --from-beginning
   #+END_SRC
   #+Caption: Topic content
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/03.05.kafka.data.png]]
* Sink Connector Deployment
** Source connector binary deployment
   Install the sink connector from [[https://github.com/luyomo/yomo-kafka-connect-jdbc][tidb jdbc sink]]  The connector is customized from confluent sink jdbc. 
   #+BEGIN_SRC
connector$ sudo wget https://github.com/luyomo/yomo-kafka-connect-jdbc/releases/download/v10.6.1/yomo-kafka-connect-jdbc-10.6.1-SNAPSHOT.jar -P /usr/share/confluent-hub-components/confluentinc-kafka-connect-jdbc/lib/
connector$ sudo systemctl restart confluent-kafka-connect
   #+END_SRC
** Sink table creation
   #+BEGIN_SRC
MySQL$ CREATE TABLE test01($oid varchar(32) primary key, t_json json);
   #+END_SRC
   #+Caption: Replication table preparation
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/04.01.tidb.table.png]]
** Sink connector creation
   + JSON config file preparation
     #+BEGIN_SRC
workstation$ more /tmp/sink.tidb.json
 {
     "name": "SINKTiDB",
     "config": {
         "connector.class": "yomo.connect.jdbc.JdbcSinkConnector",
         "connection.url": "jdbc:mysql://mgtest-a17c389fc540df4b.elb.us-east-1.amazonaws.com:4000/test?stringtype=unspecified",
         "connection.user": "root",
         "connection.password": "",
         "topics": "mongo2tidb.test.test01",
         "insert.mode": "upsert",
         "delete.enabled": "true",
         "dialect.name":"MySqlDatabaseDialect",
         "table.name.format":"test.test01",
         "pk.mode": "record_key",
         "auto.create":"false",
         "auto.evolve":"false"
     }
 }
     #+END_SRC
   + Create sink connector instance
     #+BEGIN_SRC
workstation$ curl -H "Content-Type: application/json" http://172.83.1.189:8083/connectors -d @"/tmp/sink.tidb.json" | jq
     #+END_SRC
   #+Caption: sink connector instance generation
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/04.02.sink.connect.png]]
   #+Caption: sink connector status
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/04.03.sink.status.png]]
** Data confirmation in the TiDB
   Verify that the data has been sinked into TiDB. The $oid from mongo is used as primary key while the content is replicated into t_json column. Need requirment for implementation.
   #+Caption: Data replication verification
   #+attr_html: :width 1000px
   [[./png/mongo2kafka2TiDB/04.03.tidb.data.png]]
* Replication confirmation
** Insert replication
   + Data insert into mongo
     #+Caption: Data insert
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/05.01.mongo.insert.png]]
   + Verify the inserted data has been replicated
     #+Caption: Insert Verification
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/05.02.tidb.data.png]]
** Update replication
   + Data update against mongo
     #+Caption: Data update
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/06.01.mongo.update.png]]
   + Verify the updated has been replicated
     #+Caption: Update Verification
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/06.02.tidb.data.png]]
** Delete replication
   + Data delete from mongo
     #+Caption: Data Delete
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/07.01.mongo.delete.png]]
     #+Caption: Data Delete
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/07.02.mongo.data.png]]
   + Verify deleted data has been replicated
     #+attr_html: :width 1000px
     [[./png/mongo2kafka2TiDB/07.03.tidb.data.png]] 


* Reference
** Mongo Data in the kafka topic
  #+BEGIN_SRC html :wrap EXPORT markdown
    /var/log/kafka/connect.log:[2022-11-10 04:02:16,710] DEBUG [SINKTiDB|task-0] jdbctask:put records: [SinkRecord{kafkaOffset=2   , timestampType=CreateTime} ConnectRecord{topic='mongo2tidb.test.products', kafkaPartition=0, key=Struct{id={"$oid": "636c77c88cc9e63511285250"}}, keySchema=Schema{mongo2tidb.test.products.Key:STRUCT}, value=Struct{after={"_id": {"$oid": "636c77c88cc9e63511285250"},"item": "card","qty": 16},source=Struct{version=1.9.7.Final,connector=mongodb,name=mongo2tidb,ts_ms=1668052936000,snapshot=false,db=test,rs=repli2tidb,collection=products,ord=1},op=c,ts_ms=1668052936272}, valueSchema=Schema{mongo2tidb.test.products.Envelope:STRUCT}, timestamp=1668052936645, headers=ConnectHeaders(headers=)}] (io.confluent.connect.jdbc.sink.JdbcSinkTask:89)
    
    [SinkRecord{kafkaOffset=7000, timestampType=CreateTime} ConnectRecord{topic='sourcepg.test.test01'    , kafkaPartition=0, key=Struct{pk_col=7001}                            , keySchema=Schema{sourcepg.test.test01.Key:STRUCT    }, value=Struct{pk_col=7001,t_json={"testKey":"testValue"},pg_timestamp=2022-11-10 00:56:06.851554}                                                                                                                                                                           , valueSchema=Schema{sourcepg.test.test01.Value:STRUCT       }, timestamp=1668041767182, headers=ConnectHeaders(headers=)}]
    
     {"item":"card","qty":16,"_id":{"$oid":"636cac7e6bce48592bfc3d75"}} (io.confluent.connect.jdbc.sink.JdbcDbWriter:129)
  #+END_SRC


** Data conversion
   | Type  | Data as mongo format                                                                | Data as confluent JDBC format                                                                                      |
   |-------+-------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------|
   | key   | Struct{id={"$oid": "636c77c88cc9e63511285250"}}                                     | Struct{$oid=636c77c88cc9e63511285250}                                                                              |
   | value | Struct{after={"_id": {"$oid": "636c77c88cc9e63511285250"},"item": "card","qty": 16} | Struct{$oid=636c77c88cc9e63511285250,t_json={"_id": {"$oid": "636c77c88cc9e63511285250"},"item": "card","qty": 16} |

** Debug mode on the kafka
  + Start jdbc sink plugin - add debug log to get key/keyschema/value/valueschema
    - Change the log mode from info to debug -> /etc/kafka/connect-log4j.properties
      #+BEGIN_SRC
#log4j.rootLogger=INFO, stdout, connectAppender
log4j.rootLogger=DEBUG, stdout, connectAppender
      #+END_SRC
  + Change the status to log only
  + Check json data from postgres
    - The source code in the JdbcSinkTask.java
      #+BEGIN_SRC
public void put(Collection<SinkRecord> records) {
    if (records.isEmpty()) {
      return;
    }
    final SinkRecord first = records.iterator().next();
    final int recordsCount = records.size();
    log.debug(
        "Received {} records. First record kafka coordinates:({}-{}-{}). Writing them to the "
        + "database...",
        recordsCount, first.topic(), first.kafkaPartition(), first.kafkaOffset()
    );
    log.debug("KeySchema: {}, object: {} ", first.keySchema(), first.key());
    log.debug("ValueSchema: {}, object: {} ", first.valueSchema(), first.value());
      #+END_SRC
    - JSON data format
      #+BEGIN_SRC
[2022-11-08 03:32:17,221] DEBUG [SINKTiDB|task-0] Received 398 records. First record kafka coordinates:(sourcepg.test.test01-0-9227). Writing them to the database... (io.confluent.connect.jdbc.sink.JdbcSinkTask:78)
[2022-11-08 03:32:17,221] DEBUG [SINKTiDB|task-0] KeySchema: Schema{sourcepg.test.test01.Key:STRUCT}, object: Struct{pk_col=9228}  (io.confluent.connect.jdbc.sink.JdbcSinkTask:83)
[2022-11-08 03:32:17,221] DEBUG [SINKTiDB|task-0] ValueSchema: Schema{sourcepg.test.test01.Value:STRUCT}, object: Struct{pk_col=9228,t_json={"testKey":"testValue"},pg_timestamp=2022-11-08 03:32:15.000388}  (io.confluent.connect.jdbc.sink.JdbcSinkTask:84)
... ...
[2022-11-08 03:32:17,222] DEBUG [SINKTiDB|task-0] UPSERT sql: insert into `test`.`test01`(`pk_col`,`t_json`,`pg_timestamp`) values(?,?,?) on duplicate key update `t_json`=values(`t_json`),`pg_timestamp`=values(`pg_timestamp`) deleteSql: DELETE FROM `test`.`test01` WHERE `pk_col` = ? meta: FieldsMetadata{keyFieldNames=[pk_col], nonKeyFieldNames=[t_json, pg_timestamp], allFields={pk_col=SinkRecordField{schema=Schema{INT64}, name='pk_col', isPrimaryKey=true}, t_json=SinkRecordField{schema=Schema{io.debezium.data.Json:STRING}, name='t_json', isPrimaryKey=false}, pg_timestamp=SinkRecordField{schema=Schema{STRING}, name='pg_timestamp', isPrimaryKey=false}}} (io.confluent.connect.jdbc.sink.BufferedRecords:130)
      #+END_SRC
  + Setup mongodb cluster(sinle node)
  + Catch data from mongo replica
  + Setup test case
  + Data conversion from mongodb to JDBC
  + Check records include multiple tables' data

* Source code analysis
  [[./png/mongo2kafka2TiDB/jdbc.sink.png]]
  #+BEGIN_COMMENT
  #+BEGIN_SRC plantuml :file ./png/mongo2kafka2TiDB/jdbc.sink.png
    JdbcSinkTask -> JdbcDbWriter: call write
    loop "All records"
      JdbcDbWriter -> BufferedRecords: Add records to record buffer
      BufferedRecords --> add
      add -> recordValidator.validate: To check
      add --> add: Set KeySchema if it is changed
      add --> add: Set delete flag if valueSchema is null
      add --> add: Flush data if schema is not changed.\nAnd push it to flushed
      == Process if schema is changed ==
      add --> add: Flush data and push it to flushed
      add --> add: SchemaPair re-initialization
      add --> add: FieldsMetadata extract
      add -> dbStructure: createOrAmendIfNecessary
      add --> add: getInsertSql
      add --> add: getDeleteSql
      add -> dbDialect: createPreparedStatement(insert)
      add -> dbDialect: statementBinder(insert)
      add -> dbDialect: createPreparedStatement(delete)
      add -> dbDialect: statementBinder(delete)
      == Complete schema change process ==
      add --> add: Add record to records
      add --> add: Flush if size exceeds batchSize
      add -> BufferedRecords: Return flushed data
    end
    loop "Per Table"
      JdbcDbWriter -> bufferByTable: Flush data into DB
      BufferedRecords --> flush
      loop "Per record"
        flush -> deleteStatementBinder: bindRecord if value is null
        flush -> updateStatementBinder: bindRecord if value is not null
        flush -> executeUpdates
        executeUpdates -> PreparedStatement: updatePreparedStatement.executeBatch 
        flush -> executeDeletes
        executeDeletes -> PreparedStatement: deletePreparedStatement.executeBatch
        flush -> BufferedRecords: Return flushedRecords
      end
    end
  #+END_SRC
  #+END_COMMENT
** bufferByTable(HashMap)
  [[./png/mongo2kafka2TiDB/bufferByTable.png]]
  #+BEGIN_COMMENT
  #+BEGIN_SRC plantuml :file ./png/mongo2kafka2TiDB/bufferByTable.png
    @startyaml
      table01:
        - record01
        - record02
        - ...
      table02:
        - record01
        - record02
        - ...

    @endyaml
  #+END_SRC
  #+END_COMMENT

** BufferedRecords
   + config
     configuration from sink connect
   + tableId
     unique table 
   + dbDialect
     Use dialect to for preparedstme
   + dbStructure
     todo
   + connection
     DB connection 
* Todo
** Deployment
   + Install mongo source connector
     # - Access the workstation
     # - From workstation to run query against component server
     - From golang, get the workstation ip address
     - User/Local Key
     - Read config file
       + OH_MY_TIUP=/.OhMyTiUP/config
         user: admin
         ssh_key: directory
       + ~/.OhMyTiUP/config
   + Install yomo-sink into connector
   + Restart the service
   + Transfer source json template to workstation
   + Transfer sink json template to workstation
