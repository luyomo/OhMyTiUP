* Flow
  ./pkg/executor/builder.go: Build
    ./pkg/executor/builder.go: buildMemTable
      ./pkg/executor/metrics_reader.go: queryMetric
        -> pkg/domain/infosync/info.go: GetPrometheusAddr
           Get the prometheus addr
           -> getGlobalInfoSyncer
             -> getPrometheusAddr
               -> getPrometheusAddrFromEtcd
        -> pkg/infoschema/metrics_schema.go: GenPromQL
           Generate the PromQL from the schema
           -> pkg/infoschema/metric_table_def.go
* Investigation
** Set prometheus endpoint in the PD
   #+BEGIN_SRC
workstation$ ETCDCTL_API=3 etcdctl get "/topology/prometheus" --endpoints 172.83.3.176:12379
MySQL [metrics_schema]> select * from uptime order by time desc limit 10; 
+----------------------------+--------------------+-------------------+-------------------+
| time                       | instance           | job               | value             |
+----------------------------+--------------------+-------------------+-------------------+
| 2023-10-22 05:02:14.671000 | 172.83.4.163:12379 | pd                |  832370.990999937 |
| 2023-10-22 05:02:14.671000 | 172.83.4.221:10081 | tidb              | 831217.7709999084 |
| 2023-10-22 05:02:14.671000 | 172.83.1.241:10081 | tidb              | 832367.2109999657 |
| 2023-10-22 05:02:14.671000 | 172.83.3.176:19100 | overwritten-nodes | 832353.8310000896 |
| 2023-10-22 05:02:14.671000 | 172.83.3.176:12379 | pd                | 832370.6410000324 |
| 2023-10-22 05:02:14.671000 | 172.83.4.163:19100 | overwritten-nodes | 832354.1809999943 |
| 2023-10-22 05:02:14.671000 | 172.83.1.60:12379  | pd                | 832371.1210000515 |
| 2023-10-22 05:02:14.671000 | 172.83.3.101:20181 | tikv              | 832370.6710000038 |
| 2023-10-22 05:02:14.671000 | 172.83.4.221:19100 | overwritten-nodes | 831216.2809998989 |
| 2023-10-22 05:02:14.671000 | 172.83.3.101:19100 | overwritten-nodes | 832353.4010000229 |
+----------------------------+--------------------+-------------------+-------------------+
10 rows in set (0.015 sec)
workstation$ ETCDCTL_API=3 etcdctl put "/topology/prometheus" '{"ip":"172.82.31.122","port":9090,"deploy_path":"/home/admin/tidb/tidb-deploy/prometheus-9090"}' --endpoints 172.83.3.176:12379
MySQL [metrics_schema]> select * from uptime order by time desc limit 10; 
+----------------------------+--------------------+-------------------+-------------------+
| time                       | instance           | job               | value             |
+----------------------------+--------------------+-------------------+-------------------+
| 2023-10-22 05:01:29.308000 | 172.83.4.163:2379  | pd                | 772678.8680000305 |
| 2023-10-22 05:01:29.308000 | 172.83.4.221:10080 | tidb              |  772674.128000021 |
| 2023-10-22 05:01:29.308000 | 172.83.1.241:10080 | tidb              | 772673.9380002022 |
| 2023-10-22 05:01:29.308000 | 172.83.3.176:9100  | overwritten-nodes | 772669.7180001736 |
| 2023-10-22 05:01:29.308000 | 172.83.3.176:2379  | pd                | 772678.4880001545 |
| 2023-10-22 05:01:29.308000 | 172.83.4.163:9100  | overwritten-nodes | 772670.0580000877 |
| 2023-10-22 05:01:29.308000 | 172.83.1.60:2379   | pd                |  772678.978000164 |
| 2023-10-22 05:01:29.308000 | 172.83.3.101:9100  | overwritten-nodes |  772669.248000145 |
| 2023-10-22 05:01:29.308000 | 172.83.4.221:9100  | overwritten-nodes |   772669.60800004 |
| 2023-10-22 05:01:29.308000 | 172.83.3.101:20180 | tikv              | 772678.3080000877 |
+----------------------------+--------------------+-------------------+-------------------+
10 rows in set (0.014 sec)
   #+END_SRC
** Check whether prometheus mestrics has cluster id (NO)
   #+BEGIN_SRC
curl http://172.82.31.122:19090/api/v1/query -d 'query=tikv_threads_state{instance="172.83.4.198:20181"}'  | jq
{
  "status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {
          "__name__": "tikv_threads_state",
          "instance": "172.83.4.198:20181",
          "job": "tikv",
          "state": "R"
        },
        "value": [
          1697983948.865,
          "1"
        ]
      },
      {
        "metric": {
          "__name__": "tikv_threads_state",
          "instance": "172.83.4.198:20181",
          "job": "tikv",
          "state": "S"
        },
        "value": [
          1697983948.865,
          "104"
        ]
      }
    ]
  }
}
   #+END_SRC
** Dashboard is not sharable
Check TiDB Cloud's dashboard that each cluster has its prometheus. The prometheus is not shared by all the instances. The remote write should be used to sync to one instance to open the service with additional cluster id. Need to confirm to R&D team.
** Where the metrics_schema is used in the TiDB
*** [[https://docs.pingcap.com/tidb/stable/sql-statement-calibrate-resource][calibrate resource]] - executor/calibrate_resource.go
    + SELECT SUM(value) FROM METRICS_SCHEMA.tikv_cpu_quota GROUP BY time ORDER BY time desc limit 1
    + SELECT SUM(value) FROM METRICS_SCHEMA.tidb_server_maxprocs GROUP BY time ORDER BY time desc limit 1
    + SELECT value FROM METRICS_SCHEMA.resource_manager_resource_unit where time >= '%s' and time <= '%s' ORDER BY time desc
    + SELECT sum(value) FROM METRICS_SCHEMA.process_cpu_usage where time >= '%s' and time <= '%s' and job like '%%%s' GROUP BY time ORDER BY time desc
*** [[https://docs.pingcap.com/tidb/stable/information-schema-inspection-result][inspection result]] - executor/inspection_result.go
    + select instance, value from metrics_schema.node_total_memory where time=now()
    + select instance, max(value) as max_usage from metrics_schema.node_memory_usage %s group by instance having max_usage >= 70
    + select instance, max(value) as max_used from metrics_schema.node_memory_swap_used %s group by instance having max_used > 0
    + select instance, device, max(value) as max_usage from metrics_schema.node_disk_usage %v and device like '/%%' group by instance, device having max_usage >=  70
    + select t1.instance, t1.max_load , 0.7*t2.cpu_count from
             (select instance,max(value) as max_load  from metrics_schema.%[1]s %[2]s group by instance) as t1 join
             (select instance,max(value) as cpu_count from metrics_schema.node_virtual_cpus %[2]s group by instance) as t2
             on t1.instance=t2.instance where t1.max_load>(0.7*t2.cpu_count)
    + select t1.job,t1.instance, t2.min_time from
         (select instance,job from metrics_schema.up %[1]s group by instance,job having max(value)-min(value)>0) as t1 join
         (select instance,min(time) as min_time from metrics_schema.up %[1]s and value=0 group by instance,job) as t2 on t1.instance=t2.instance order by job
    + select t1.status_address, t1.cpu, (t2.value * %[2]f) as threshold, t2.value from
                 (select status_address, max(sum_value) as cpu from (select instance as status_address, sum(value) as sum_value from metrics_schema.tikv_thread_cpu %[4]s and name    like '%[1]s' group by instance, time) as tmp group by tmp.status_address) as t1 join
                 (select instance, value from information_schema.cluster_config where type='tikv' and %[5]s = '%[3]s') as t2 join
                 (select instance,status_address from information_schema.cluster_info where type='tikv') as t3
                 on t1.status_address=t3.status_address and t2.instance=t3.instance where t1.cpu > (t2.value * %[2]f)
    + SELECT t1.address,
             max(t1.value),
             t2.address,
             min(t2.value),
             max((t1.value-t2.value)/t1.value) AS ratio
         FROM metrics_schema.pd_scheduler_store_status t1
         JOIN metrics_schema.pd_scheduler_store_status t2 %s
             AND t1.type='%s'
             AND t1.time = t2.time
             AND t1.type=t2.type
             AND t1.address != t2.address
             AND (t1.value-t2.value)/t1.value>%v
             AND t1.value > 0
         GROUP BY  t1.address,t2.address
         ORDER BY  ratio desc
     + select instance, sum(value) as sum_value from metrics_schema.pd_region_health %s and
         type in ('extra-peer-region-count','learner-peer-region-count','pending-peer-region-count') having sum_value>100
     + select address, max(value) from metrics_schema.pd_scheduler_store_status %s and type='region_count' and value > 20000 group by address
     + select address,min(value) as mi,max(value) as mx from metrics_schema.pd_scheduler_store_status %s and type='leader_count' group by address having mx-mi>%v
     + select time, value from metrics_schema.pd_scheduler_store_status %s and type='leader_count' and address = '%s' order by time
*** metrics profiling(curl http://tidbnode:10081/metrics/profile) - server/http_status.go
    + select sum(value), '' from `metrics_schema`.`%v_total_count` %v
    + select sum(value), `%[3]s` from `metrics_schema`.`%[1]s_total_count` %[2]s group by `%[3]s` having sum(value) > 0
    + select sum(value), '' from `metrics_schema`.`%v_total_time` %v
    + select avg(value), '' from `metrics_schema`.`%v_duration` %v
    + select avg(value), `%[3]s` from `metrics_schema`.`%[1]s_duration` %[2]s group by `%[3]s
* ng-monitoring deployment
  Download bin ng-monitoring-server first.
  #+BEGIN_SRC
workstation$ more ngmonitoring.toml
# NG Monitoring Server Configuration.

# Server address.
address = "0.0.0.0:22021"
advertise-address = "172.82.31.122:22021"

[log]
# Log path
path = "/home/tidb/deploy/log"

# Log level: INFO, WARN, ERROR
level = "INFO"

[pd]
# Addresses of PD instances within the TiDB cluster. Multiple addresses are separated by commas, e.g. "10.0.0.1:2379","10.0.0.2:2379"
endpoints = ["172.83.3.176:2379","172.83.1.60:2379","172.83.4.163:2379"]

[storage]
path = "/home/tidb/deploy/ngmonitoring"

workstation$ ng-wrapper.sh
#!/bin/bash

# WARNING: This file was auto-generated to restart ng-monitoring when fail. 
#          Do not edit! All your edit might be overwritten!

while true
do
    /tmp/tidb/ng-monitoring-server         --config /tmp/tidb/ngmonitoring.toml >/dev/null 2>&1
    sleep 15s
done

workstation$ ng-wrapper.sh
  #+END_SRC
* tidashboard deployment
* Todo
** atomic.Value - pkg/domain/infosync/info.go
   #+BEGIN_SRC
var globalInfoSyncer atomic.Value

func getGlobalInfoSyncer() (*InfoSyncer, error) {
        v := globalInfoSyncer.Load()
        if v == nil {
                return nil, errors.New("infoSyncer is not initialized")
        }
        return v.(*InfoSyncer), nil
}

func setGlobalInfoSyncer(is *InfoSyncer) {
        globalInfoSyncer.Store(is)
}
   #+END_SRC
** set @@tidb_enable_ddl=false -> can not disable ddl owner when it is the only one tidb instance
   pkg/domain/infosync/info.go
   pd: /tidb/server/info

   #+BEGIN_SRC
workstation$ ETCDCTL_API=3 etcdctl get "/tidb/server/info" --endpoints 172.83.1.60:12379 --prefix=true
{"version":"5.7.25-TiDB-v7.1.0","git_hash":"635a4362235e8a3c0043542e629532e3c7bb2756","ddl_id":"31aaddfc-640a-4139-bc19-a2fae7723ed0","ip":"172.83.4.221","listening_port":4001,"status_port":10081,"lease":"45s","binlog_status":"Off","start_timestamp":1697119717,"labels":{},"server_id":1932681}
/tidb/server/info/357284c3-53b3-4594-af65-ac99d4a6d8b4
{"version":"5.7.25-TiDB-v7.1.0","git_hash":"635a4362235e8a3c0043542e629532e3c7bb2756","ddl_id":"357284c3-53b3-4594-af65-ac99d4a6d8b4","ip":"172.83.1.241","listening_port":4001,"status_port":10081,"lease":"45s","binlog_status":"Off","start_timestamp":1697118575,"labels":{},"server_id":3740511}
   #+END_SRC
*** Failed to fetch the /tidb/server/info
    #+BEGIN_SRC
workstation$ ETCDCTL_API=3 etcdctl get "/tidb/server/info" --endpoints 172.83.1.60:2379 --prefix=true
    #+END_SRC
